{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations of word2vec\n",
    "1. No sentence representations -- taking the average pre-trained word vector is popular. But, it does not work very well.\n",
    "2. Not exploiting morphology -- words with same radicals do not share parameters -- e.g., disastrous and disaster will be 2 different words, mangera and mangerai are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of fastText library\n",
    "* Unified framework for-\n",
    "  * Text representation\n",
    "    * Word representation (with character-level features with character n-grams)\n",
    "  * Text classification\n",
    "* Core of the library: given a set of indiceas -> predict an index\n",
    "* cbow, skip-gram and bow text classification are instances of this model.\n",
    "  * CBOW: given many words, predict the word\n",
    "  * skip-gram: given word, predict the word\n",
    "  * text-classification: given many words, predict the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The CBOW and the skipgram models\n",
    "![cbow+skipgram](figs/cbow_and_skipgram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW\n",
    "- You are given a context (i.e., a group of surrounding words around a word -- `n` words preceding and `n` words succeeding the word), predict the word in the middle.\n",
    "  ![cbow](figs/cbow.png)\n",
    "- Model the probability of a `word` given a context:\n",
    "  $$\n",
    "  p(w|C) = \\dfrac{e^{h^T_Cv_w}}{\\sum_{k=1}^K e^{h^T_Cv_k}}\n",
    "  $$\n",
    "- where, feature for context $C: h_C$\n",
    "- classifier for word, $w: v_w$\n",
    "- Continuous `Bag of Words`, $h_C = \\sum_{c\\in C}x_C$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-gram\n",
    "* You are given the word in the middle, predict the surrounding words.\n",
    "  ![skipgram](figs/skipgram.png)\n",
    "* Model probability of a `context word` given a word: $p(c|w) = \\dfrac{e^{x^T_wv_c}}{\\sum_{k=1}^Ke^{x^T_wv_k}}$\n",
    "  * where, feature for word $w$: $x_w$,\n",
    "  * classifier for word $c$: $v_c$\n",
    "  * Word vectors, $x_w \\in \\mathbb{R}^d$\n",
    "* Minimize a `negative log likelihood`: \n",
    "  * Given, a stream of words $(w_1, \\cdots, w_t, \\cdots, w_T)$\n",
    "  * Objective: \n",
    "  $$\n",
    "  \\min_{x, v} \\quad -\\sum_{t=1}^T\\sum_{c\\in C_t} \\log \\dfrac{e^{x^T_{w_t}v_c}}{\\sum_{k=1}^Ke^{x^T_{w_t}v_k}}\n",
    "  $$\n",
    "  * The denominator sum is computationally intensive! The above sum hids `co-occurrence counts`\n",
    "* Approximations to the loss:\n",
    "  * Replace the multi-class loss by a set of binary logistic losses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastText\n",
    "* Both models are instances of a broader set of models\n",
    "* Different input and output dictionarires.\n",
    "* Common core but different pooling strategies.\n",
    "* Efficient and modular C++ implementation.\n",
    "* Allows easy building of extensions by writing own pooling.\n",
    "* Ease of use is at the core of the library\n",
    "  * `./fasttext supervised -input data/dbpedia.train -output data/dbpedia`\n",
    "  * `./fasttext test data/dbpedia.bin data/dbpedia.test`\n",
    "* Model probability of a `label` given a paragraph:\n",
    "  * feature for paragraph $P: h_P$\n",
    "  * classifier for label $l: v_l$\n",
    "  $$\n",
    "  p(l|P) = \\dfrac{e^{h^T_Pv_l}}{\\sum_{k=1}^Ke^{h^T_Pv_k}}\n",
    "  $$\n",
    "  * paragraph feature $h_P = \\sum_{w\\in P}x_w$\n",
    "  * Word vectors are latent and not useful *per se*\n",
    "  * If scarce supervised data, use pre-trained word vectors.\n",
    "* Represent words as sum of its `character n-grams`, and the word itself.\n",
    "  * Used hashed dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, let's code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing fastText\n",
    "Reference [https://fasttext.cc/docs/en/supervised-tutorial.html](https://fasttext.cc/docs/en/supervised-tutorial.html)\n",
    "* `$ wget https://github.com/facebookresearch/fastText/archive/v0.9.2.zip`\n",
    "* `$ unzip v0.9.2.zip`\n",
    "* `$ cd fastText-0.9.2`\n",
    "* `$ make`\n",
    "* `$ pip install .`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the help function will show high level documentation of the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module fasttext.FastText in fasttext:\n",
      "\n",
      "NAME\n",
      "    fasttext.FastText\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright (c) 2017-present, Facebook, Inc.\n",
      "    # All rights reserved.\n",
      "    #\n",
      "    # This source code is licensed under the MIT license found in the\n",
      "    # LICENSE file in the root directory of this source tree.\n",
      "\n",
      "FUNCTIONS\n",
      "    cbow(*kargs, **kwargs)\n",
      "    \n",
      "    eprint(*args, **kwargs)\n",
      "    \n",
      "    load_model(path)\n",
      "        Load a model given a filepath and return a model object.\n",
      "    \n",
      "    read_args(arg_list, arg_dict, arg_names, default_values)\n",
      "    \n",
      "    skipgram(*kargs, **kwargs)\n",
      "    \n",
      "    supervised(*kargs, **kwargs)\n",
      "    \n",
      "    tokenize(text)\n",
      "        Given a string of text, tokenize it and return a list of tokens\n",
      "    \n",
      "    train_supervised(*kargs, **kwargs)\n",
      "        Train a supervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input file must must contain at least one label per line. For an\n",
      "        example consult the example datasets which are part of the fastText\n",
      "        repository such as the dataset pulled by classification-example.sh.\n",
      "    \n",
      "    train_unsupervised(*kargs, **kwargs)\n",
      "        Train an unsupervised model and return a model object.\n",
      "        \n",
      "        input must be a filepath. The input text does not need to be tokenized\n",
      "        as per the tokenize function, but it must be preprocessed and encoded\n",
      "        as UTF-8. You might want to consult standard preprocessing scripts such\n",
      "        as tokenizer.perl mentioned here: http://www.statmt.org/wmt07/baseline.html\n",
      "        \n",
      "        The input field must not contain any labels or use the specified label prefix\n",
      "        unless it is ok for those words to be ignored. For an example consult the\n",
      "        dataset pulled by the example script word-vector-example.sh, which is\n",
      "        part of the fastText repository.\n",
      "\n",
      "DATA\n",
      "    BOW = '<'\n",
      "    EOS = '</s>'\n",
      "    EOW = '>'\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    displayed_errors = {}\n",
      "    division = _Feature((2, 2, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0), 1310...\n",
      "    print_function = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', 0)...\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "    unsupervised_default = {'autotuneDuration': 300, 'autotuneMetric': 'f1...\n",
      "\n",
      "FILE\n",
      "    /home/ashiskb/venv-tf2/lib/python3.10/site-packages/fasttext/FastText.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fasttext.FastText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our first classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset below looks like:\n",
    "\n",
    "`__label__sauce __label__cheese How much does potato starch affect a cheese sauce recipe?`\n",
    "\n",
    "`__label__food-safety __label__acidity Dangerous pathogens capable of growing in acidic environments`\n",
    "\n",
    "`__label__cast-iron __label__stove How do I cover up the white spots on my cast iron stove?`\n",
    "\n",
    "`__label__restaurant Michelin Three Star Restaurant; but if the chef is not there`\n",
    "\n",
    "`__label__knife-skills __label__dicing Without knife skills, how can I quickly and accurately dice vegetables?`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `$ head -n 12404 cooking.stackexchange.txt > cooking.stackexchange.12404.train`\n",
    "* `$ tail -n 3000 cooking.stackexchange.txt > cooking.stackexchange.3000.valid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  14543\n",
      "Number of labels: 735\n",
      "Progress: 100.0% words/sec/thread:   40933 lr:  0.000000 avg.loss: 10.033125 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "model = fasttext.train_supervised(input='../datasets/cooking.stackexchange/cooking.stackexchange.12404.train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model('../model_files/model_cooking_stackexchange_fasttext.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__baking',), array([0.06053093]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now, we can test the classifier\n",
    "model.predict(\"Which baking dish is best to bake a banana bread?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__baking',), array([0.06984674]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"Why not put knives in the dishwasher?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 0.14, 0.060544904137235116)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"../datasets/cooking.stackexchange/cooking.stackexchange.3000.valid\")\n",
    "\n",
    "#returns: num_of_samples, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 0.0672, 0.14530776992936428)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"../datasets/cooking.stackexchange/cooking.stackexchange.3000.valid\", k=5)\n",
    "\n",
    "#returns: num_of_samples, precision@k, recall@k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the model better\n",
    "The model obtained by running fastText with the default arguments is pretty bad at classifying new questions. Let's try to improve the performance, by changing the default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing the data\n",
    "* Looking at the data, we observe that some words contain uppercase letter or punctuation. \n",
    "* One of the first step to improve the performance of our model is to apply some simple pre-processing. \n",
    "* A crude normalization can be obtained using command line tools such as `sed` and `tr`:\n",
    "  * `$ cat cooking.stackexchange.txt | sed -e \"s/\\([.\\!?,'/()]\\)/ \\1 /g\" | tr \"[:upper:]\" \"[:lower:]\" > cooking.stackexchange.preprocessed.txt`\n",
    "  * `$ head -n 12404 cooking.stackexchange.preprocessed.txt > cooking.stackexchange.preprocessed.12404.train`\n",
    "  * `$ tail -n 3000 cooking.stackexchange.preprocessed.txt > cooking.stackexchange.preprocessed.3000.valid`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's train a new model on the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  8952\n",
      "Number of labels: 735\n",
      "Progress: 100.0% words/sec/thread:   44573 lr:  0.000000 avg.loss: 10.008055 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(input='../datasets/cooking.stackexchange/cooking.stackexchange.preprocessed.12404.train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that thanks to the pre-processing, the vocabulary is smaller (from 14k words to 9k). The precision is also starting to go up by 4%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 0.16866666666666666, 0.0729421940320023)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(\"../datasets/cooking.stackexchange/cooking.stackexchange.preprocessed.3000.valid\")\n",
    "\n",
    "#returns: num_of_samples, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More epochs and larger learning rate\n",
    "* By default, `fastText` sees each training example only five times during training, which is pretty small, given that our training set only have 12k training examples. The number of times each examples is seen (also known as the number of epochs), can be increased using the `-epoch` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  8952\n",
      "Number of labels: 735\n",
      "Progress: 100.0% words/sec/thread:   44510 lr:  0.000000 avg.loss:  7.193739 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(input=\"../datasets/cooking.stackexchange/cooking.stackexchange.preprocessed.12404.train\", epoch=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 0.5136666666666667, 0.22214213637018884)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now, test the model\n",
    "model.test(\"../datasets/cooking.stackexchange/cooking.stackexchange.preprocessed.3000.valid\")\n",
    "\n",
    "#returns: num_of_samples, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is much better! \n",
    "* Another way to change the learning speed of our model is to increase (or decrease) the learning rate of the algorithm. This corresponds to how much the model changes after processing each example. A learning rate of 0 would mean that the model does not change at all, and thus, does not learn anything. Good values of the learning rate are in the range 0.1 - 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  8952\n",
      "Number of labels: 735\n",
      "Progress: 100.0% words/sec/thread:   44533 lr:  0.000000 avg.loss:  4.380679 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(input=\"../datasets/cooking.stackexchange/cooking.stackexchange.preprocessed.12404.train\", lr=1.0, epoch=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 0.581, 0.2512613521695257)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now, test the model\n",
    "model.test(\"../datasets/cooking.stackexchange/cooking.stackexchange.preprocessed.3000.valid\")\n",
    "\n",
    "#returns: num_of_samples, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving with word n-grams\n",
    "* Finally, we can improve the performance of a model by using word bigrams, instead of just unigrams. This is especially important for classification problems where word order is important, such as sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  8952\n",
      "Number of labels: 735\n",
      "Progress: 100.0% words/sec/thread:   44212 lr:  0.000000 avg.loss:  3.066071 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(input=\"../datasets/cooking.stackexchange/cooking.stackexchange.preprocessed.12404.train\", lr=1.0, epoch=25, wordNgrams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 0.6086666666666667, 0.2632261784633127)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now, test the model\n",
    "model.test(\"../datasets/cooking.stackexchange/cooking.stackexchange.preprocessed.3000.valid\")\n",
    "\n",
    "#returns: num_of_samples, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With a few steps, we were able to go from a precision at one of 12.4% to 59.9%. Important steps included:\n",
    "* **preprocessing the data**\n",
    "    * changing the number of epochs (using the option `-epoch`, standard range [5 - 50]) ;\n",
    "    * changing the learning rate (using the option `-lr`, standard range [0.1 - 1.0]) ;\n",
    "    * using word n-grams (using the option `-wordNgrams`, standard range [1 - 5])."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling things up\n",
    "* Since we are training our model on a few thousands of examples, the training only takes a few seconds. But training models on larger datasets, with more labels can start to be too slow. A potential solution to make the training faster is to use the hierarchical softmax, instead of the regular softmax. This can be done with the option `-loss hs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  8952\n",
      "Number of labels: 735\n",
      "Progress: 100.0% words/sec/thread: 1438728 lr:  0.000000 avg.loss:  2.262096 ETA:   0h 0m 0s avg.loss:  2.262096 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(input=\"../datasets/cooking.stackexchange/cooking.stackexchange.preprocessed.12404.train\", lr=1.0, epoch=25, wordNgrams=2, bucket=200000, dim=50, loss='hs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training should now take less than a second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label classification\n",
    "* When we want to assign a document to multiple labels, we can still use the softmax loss and play with the parameters for prediction, namely the number of labels to predict and the threshold for the predicted probability. However playing with these arguments can be tricky and unintuitive since the probabilities must sum to 1.\n",
    "* A convenient way to handle multiple labels is to use independent binary classifiers for each label. This can be done with `-loss one-vs-all` or `-loss ova`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  8952\n",
      "Number of labels: 735\n",
      "Progress: 100.0% words/sec/thread:   82759 lr:  0.000000 avg.loss:  4.038188 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(input=\"../datasets/cooking.stackexchange/cooking.stackexchange.preprocessed.12404.train\", lr=0.5, epoch=25, wordNgrams=2, bucket=200000, dim=50, loss='ova')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is a good idea to decrease the learning rate compared to other loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 0.6063333333333333, 0.2622170967276921)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now, test the model\n",
    "model.test(\"../datasets/cooking.stackexchange/cooking.stackexchange.preprocessed.3000.valid\")\n",
    "\n",
    "#returns: num_of_samples, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 0.003146031746031746, 1.0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now, test the model\n",
    "model.test(\"../datasets/cooking.stackexchange/cooking.stackexchange.preprocessed.3000.valid\", k=-1)\n",
    "\n",
    "#returns: num_of_samples, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look on our predictions, we want as many prediction as possible (argument -1) and we want only labels with probability higher or equal to 0.5 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('__label__baking',\n",
       "  '__label__bread',\n",
       "  '__label__equipment',\n",
       "  '__label__bananas'),\n",
       " array([1.00001001, 0.98935753, 0.97069776, 0.83974397]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(\"Which baking dish is best to bake a banana bread ?\", k=-1, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate our results with the test function:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv-tf2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68af2642b5728495b2dc6c854cc189f16b92a273802cb2ba2e094b90dc703a77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
